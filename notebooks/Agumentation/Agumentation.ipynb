{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agumentation\n",
    "#### From the profiling of the dataset it is evident that a lot of words are not present in the italian vocabolary\n",
    "#### For this reason, and because the data is highly unbalanced, agumentation is needed to increase italian datapoints and\n",
    "####  words in its vocabolary to reduce eventual out-of-vocabolary inputs\n",
    "\n",
    "##### LOGIC\n",
    "##### For each [langueage selected] text in the dataset: \n",
    "#####   we translate the phrase and check its words\n",
    "#####   if the words are all present in the italian vocabolary:\n",
    "#####       we do not insert it in the dataset \n",
    "#####       (this is because inserting the phrase will not generate new knowlage)\n",
    "#####       (We could say that the informative content of the new phrase is 0)\n",
    "#####   else, \n",
    "#####   if there is at least one new word\n",
    "####        we insert the new phrase\n",
    "\n",
    "##### The dataset holds 698 italian text and 9639 not italian\n",
    "##### We performe agumentation in the following way:\n",
    "##### 1. Agumentation with english phrases from Lang_det_parsed.csv -> Dataset_agumented1.csv\n",
    "##### 2. Agumentation with french phrases from Dataset_agumented1.csv-> Dataset_agumented2.csv\n",
    "##### 3. Agumentation with spanish phrases from Dataset_agumented2.csv-> Dataset_agumented3.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### We will use GoogleTranlator api (sorry =) )\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('..\\\\..\\\\Dataset\\\\Lang_det_parsed.csv')   #the parsed dataset\n",
    "words=pd.read_csv('..\\\\..\\\\Dataset\\\\words.csv')     #csv holdeing all words in the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1385,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we retrive english phrases\n",
    "english_text=dataset[dataset['Language']=='English']['Text']\n",
    "english_text.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'abstract',\n",
       " 'academy',\n",
       " 'acceso',\n",
       " 'accurate',\n",
       " 'ad',\n",
       " 'agosto',\n",
       " 'ai',\n",
       " 'al',\n",
       " 'alan',\n",
       " 'algoritmo',\n",
       " 'all',\n",
       " 'alla',\n",
       " 'alle',\n",
       " 'alta',\n",
       " 'alto',\n",
       " 'america',\n",
       " 'amo',\n",
       " 'an',\n",
       " 'and',\n",
       " 'andrea',\n",
       " 'android',\n",
       " 'anno',\n",
       " 'ars',\n",
       " 'arte',\n",
       " 'arthur',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'atroce',\n",
       " 'aumento',\n",
       " 'austria',\n",
       " 'automaticamente',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'backpropagation',\n",
       " 'bancario',\n",
       " 'barack',\n",
       " 'basa',\n",
       " 'base',\n",
       " 'bayesiana',\n",
       " 'bbc',\n",
       " 'bel',\n",
       " 'bell',\n",
       " 'ben',\n",
       " 'biblioteca',\n",
       " 'bibliotecario',\n",
       " 'biografia',\n",
       " 'blue',\n",
       " 'bomis',\n",
       " 'bradley',\n",
       " 'brand',\n",
       " 'bravo',\n",
       " 'breiman',\n",
       " 'britannica',\n",
       " 'brockhaus',\n",
       " 'buon',\n",
       " 'businessweek',\n",
       " 'bysa',\n",
       " 'c',\n",
       " 'california',\n",
       " 'campana',\n",
       " 'campo',\n",
       " 'canada',\n",
       " 'canto',\n",
       " 'cara',\n",
       " 'carne',\n",
       " 'carta',\n",
       " 'casa',\n",
       " 'casi',\n",
       " 'caso',\n",
       " 'catch',\n",
       " 'categoria',\n",
       " 'categorie',\n",
       " 'causa',\n",
       " 'cd',\n",
       " 'ce',\n",
       " 'cell',\n",
       " 'cena',\n",
       " 'centrale',\n",
       " 'centro',\n",
       " 'ceo',\n",
       " 'certamente',\n",
       " 'certo',\n",
       " 'check',\n",
       " 'cielo',\n",
       " 'ciffolilli',\n",
       " 'cima',\n",
       " 'cinema',\n",
       " 'circa',\n",
       " 'cita',\n",
       " 'classe',\n",
       " 'clic',\n",
       " 'click',\n",
       " 'cluster',\n",
       " 'clustering',\n",
       " 'come',\n",
       " 'commerciale',\n",
       " 'commons',\n",
       " 'community',\n",
       " 'completamente',\n",
       " 'comportamento',\n",
       " 'comprensiva',\n",
       " 'computational',\n",
       " 'computer',\n",
       " 'computing',\n",
       " 'con',\n",
       " 'confuso',\n",
       " 'consenso',\n",
       " 'considera',\n",
       " 'consiste',\n",
       " 'consulta',\n",
       " 'contrario',\n",
       " 'controverse',\n",
       " 'controversia',\n",
       " 'cool',\n",
       " 'copie',\n",
       " 'copyleft',\n",
       " 'copyright',\n",
       " 'coronation',\n",
       " 'corse',\n",
       " 'corso',\n",
       " 'cosa',\n",
       " 'crea',\n",
       " 'creando',\n",
       " 'create',\n",
       " 'creative',\n",
       " 'critici',\n",
       " 'crossover',\n",
       " 'cultura',\n",
       " 'cunningham',\n",
       " 'curriculum',\n",
       " 'cyberarts',\n",
       " 'd',\n",
       " 'da',\n",
       " 'dale',\n",
       " 'data',\n",
       " 'database',\n",
       " 'dataset',\n",
       " 'dato',\n",
       " 'del',\n",
       " 'delhi',\n",
       " 'dell',\n",
       " 'denota',\n",
       " 'dentro',\n",
       " 'dette',\n",
       " 'deve',\n",
       " 'devo',\n",
       " 'dia',\n",
       " 'dice',\n",
       " 'difficile',\n",
       " 'diffuse',\n",
       " 'digitale',\n",
       " 'dilemma',\n",
       " 'dio',\n",
       " 'dire',\n",
       " 'disciplina',\n",
       " 'discipline',\n",
       " 'discovery',\n",
       " 'dispositivo',\n",
       " 'dispute',\n",
       " 'disse',\n",
       " 'diverse',\n",
       " 'dna',\n",
       " 'do',\n",
       " 'dominio',\n",
       " 'donna',\n",
       " 'due',\n",
       " 'durante',\n",
       " 'dvd',\n",
       " 'e',\n",
       " 'economia',\n",
       " 'economist',\n",
       " 'edit',\n",
       " 'editor',\n",
       " 'eh',\n",
       " 'electronica',\n",
       " 'elemento',\n",
       " 'ellie',\n",
       " 'email',\n",
       " 'emigh',\n",
       " 'encarta',\n",
       " 'enciclopedia',\n",
       " 'enciclopedicamente',\n",
       " 'encyclopedia',\n",
       " 'encyclopædia',\n",
       " 'enorme',\n",
       " 'enwikipediaorg',\n",
       " 'epic',\n",
       " 'episodio',\n",
       " 'equivalente',\n",
       " 'era',\n",
       " 'es',\n",
       " 'essa',\n",
       " 'esse',\n",
       " 'everything',\n",
       " 'evitando',\n",
       " 'examples',\n",
       " 'extra',\n",
       " 'facile',\n",
       " 'facilmente',\n",
       " 'factor',\n",
       " 'fair',\n",
       " 'false',\n",
       " 'falso',\n",
       " 'famoso',\n",
       " 'farina',\n",
       " 'fase',\n",
       " 'feedback',\n",
       " 'festa',\n",
       " 'festival',\n",
       " 'figure',\n",
       " 'film',\n",
       " 'filosofia',\n",
       " 'fin',\n",
       " 'fine',\n",
       " 'fino',\n",
       " 'fluente',\n",
       " 'flummox',\n",
       " 'fondo',\n",
       " 'fonte',\n",
       " 'forma',\n",
       " 'formalmente',\n",
       " 'formato',\n",
       " 'forte',\n",
       " 'fortemente',\n",
       " 'fortuna',\n",
       " 'fosse',\n",
       " 'foto',\n",
       " 'foundation',\n",
       " 'frances',\n",
       " 'francia',\n",
       " 'frank',\n",
       " 'frase',\n",
       " 'futura',\n",
       " 'futuro',\n",
       " 'g',\n",
       " 'generalmente',\n",
       " 'genere',\n",
       " 'geometria',\n",
       " 'geometry',\n",
       " 'gfdl',\n",
       " 'giro',\n",
       " 'globale',\n",
       " 'gmat',\n",
       " 'golden',\n",
       " 'google',\n",
       " 'grado',\n",
       " 'grammatica',\n",
       " 'gran',\n",
       " 'grande',\n",
       " 'gratuita',\n",
       " 'group',\n",
       " 'guardian',\n",
       " 'guerra',\n",
       " 'guide',\n",
       " 'ha',\n",
       " 'hard',\n",
       " 'hardware',\n",
       " 'hegel',\n",
       " 'heller',\n",
       " 'herring',\n",
       " 'hey',\n",
       " 'hg',\n",
       " 'hinton',\n",
       " 'ho',\n",
       " 'hoiberg',\n",
       " 'hopfield',\n",
       " 'html',\n",
       " 'i',\n",
       " 'ia',\n",
       " 'idea',\n",
       " 'idee',\n",
       " 'identifica',\n",
       " 'il',\n",
       " 'ilp',\n",
       " 'impact',\n",
       " 'importa',\n",
       " 'importante',\n",
       " 'importasse',\n",
       " 'importava',\n",
       " 'in',\n",
       " 'incomplete',\n",
       " 'indica',\n",
       " 'inductive',\n",
       " 'informatica',\n",
       " 'information',\n",
       " 'informativa',\n",
       " 'input',\n",
       " 'insulto',\n",
       " 'intelligence',\n",
       " 'intelligente',\n",
       " 'intento',\n",
       " 'interessante',\n",
       " 'interesse',\n",
       " 'international',\n",
       " 'interne',\n",
       " 'internet',\n",
       " 'interno',\n",
       " 'interrompendo',\n",
       " 'inutile',\n",
       " 'iphone',\n",
       " 'italiana',\n",
       " 'italiano',\n",
       " 'jimmy',\n",
       " 'john',\n",
       " 'jordan',\n",
       " 'joseph',\n",
       " 'knowledgebased',\n",
       " 'l',\n",
       " 'la',\n",
       " 'largo',\n",
       " 'larry',\n",
       " 'latina',\n",
       " 'latine',\n",
       " 'le',\n",
       " 'learning',\n",
       " 'lei',\n",
       " 'lenta',\n",
       " 'lentamente',\n",
       " 'lento',\n",
       " 'leo',\n",
       " 'li',\n",
       " 'libre',\n",
       " 'libro',\n",
       " 'light',\n",
       " 'line',\n",
       " 'link',\n",
       " 'linus',\n",
       " 'liste',\n",
       " 'lo',\n",
       " 'logic',\n",
       " 'logo',\n",
       " 'lucro',\n",
       " 'lui',\n",
       " 'là',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'machine',\n",
       " 'machinery',\n",
       " 'madre',\n",
       " 'magazine',\n",
       " 'mai',\n",
       " 'male',\n",
       " 'mamma',\n",
       " 'mano',\n",
       " 'mantiene',\n",
       " 'marian',\n",
       " 'marion',\n",
       " 'marketing',\n",
       " 'markov',\n",
       " 'markup',\n",
       " 'marzo',\n",
       " 'mass',\n",
       " 'massa',\n",
       " 'materiale',\n",
       " 'matrimonio',\n",
       " 'mchenry',\n",
       " 'me',\n",
       " 'media',\n",
       " 'mediawiki',\n",
       " 'medio',\n",
       " 'melissa',\n",
       " 'melli',\n",
       " 'mellie',\n",
       " 'mentalmente',\n",
       " 'mente',\n",
       " 'messenger',\n",
       " 'mi',\n",
       " 'michael',\n",
       " 'microsoft',\n",
       " 'mille',\n",
       " 'mining',\n",
       " 'ministro',\n",
       " 'minuto',\n",
       " 'mio',\n",
       " 'mirror',\n",
       " 'mitchell',\n",
       " 'mobile',\n",
       " 'modo',\n",
       " 'momento',\n",
       " 'mondiale',\n",
       " 'motivo',\n",
       " 'multilingue',\n",
       " 'nah',\n",
       " nan,\n",
       " 'narcis',\n",
       " 'narcisa',\n",
       " 'narcisismo',\n",
       " 'narciso',\n",
       " 'narcissa',\n",
       " 'narcisse',\n",
       " 'national',\n",
       " 'natura',\n",
       " 'nature',\n",
       " 'ne',\n",
       " 'necessariamente',\n",
       " 'negativo',\n",
       " 'news',\n",
       " 'nica',\n",
       " 'no',\n",
       " 'nome',\n",
       " 'non',\n",
       " 'norma',\n",
       " 'normalmente',\n",
       " 'nota',\n",
       " 'note',\n",
       " 'novembre',\n",
       " 'nupedia',\n",
       " 'né',\n",
       " 'o',\n",
       " 'obama',\n",
       " 'occupa',\n",
       " 'of',\n",
       " 'offline',\n",
       " 'offre',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'on',\n",
       " 'online',\n",
       " 'open',\n",
       " 'opera',\n",
       " 'origine',\n",
       " 'oro',\n",
       " 'output',\n",
       " 'overfitting',\n",
       " 'oxford',\n",
       " 'p',\n",
       " 'padre',\n",
       " 'pae',\n",
       " 'pagina',\n",
       " 'paideia',\n",
       " 'paradigma',\n",
       " 'parlamento',\n",
       " 'parole',\n",
       " 'parte',\n",
       " 'passo',\n",
       " 'password',\n",
       " 'pattern',\n",
       " 'pdf',\n",
       " 'pedia',\n",
       " 'pena',\n",
       " 'penso',\n",
       " 'per',\n",
       " 'perceptrons',\n",
       " 'performance',\n",
       " 'permanente',\n",
       " 'persona',\n",
       " 'personalmente',\n",
       " 'philip',\n",
       " 'pizza',\n",
       " 'poco',\n",
       " 'positivo',\n",
       " 'possa',\n",
       " 'posso',\n",
       " 'precisa',\n",
       " 'preciso',\n",
       " 'premio',\n",
       " 'preprocessing',\n",
       " 'presente',\n",
       " 'preso',\n",
       " 'prestigio',\n",
       " 'primaria',\n",
       " 'principalmente',\n",
       " 'principe',\n",
       " 'principio',\n",
       " 'prix',\n",
       " 'problema',\n",
       " 'proceedings',\n",
       " 'processo',\n",
       " 'profit',\n",
       " 'programma',\n",
       " 'programming',\n",
       " 'progress',\n",
       " 'pronto',\n",
       " 'pronuncia',\n",
       " 'propose',\n",
       " 'proposta',\n",
       " 'protein',\n",
       " 'prova',\n",
       " 'prove',\n",
       " 'proviene',\n",
       " 'publishing',\n",
       " 'punto',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'quanto',\n",
       " 'quel',\n",
       " 'quelle',\n",
       " 'quest',\n",
       " 'qui',\n",
       " 'raramente',\n",
       " 'realmente',\n",
       " 'records',\n",
       " 'regalo',\n",
       " 'relativamente',\n",
       " 'rende',\n",
       " 'rendezvous',\n",
       " 'repository',\n",
       " 'resa',\n",
       " 'retrieval',\n",
       " 'robert',\n",
       " 'robot',\n",
       " 'rock',\n",
       " 'routine',\n",
       " 'rumelhart',\n",
       " 'russo',\n",
       " 'sa',\n",
       " 'sai',\n",
       " 'sala',\n",
       " 'salto',\n",
       " 'samuel',\n",
       " 'sanger',\n",
       " 'sapere',\n",
       " 'school',\n",
       " 'science',\n",
       " 'sciences',\n",
       " 'se',\n",
       " 'sei',\n",
       " 'seigenthaler',\n",
       " 'sempre',\n",
       " 'senso',\n",
       " 'september',\n",
       " 'sera',\n",
       " 'seria',\n",
       " 'serio',\n",
       " 'server',\n",
       " 'si',\n",
       " 'siciliano',\n",
       " 'siete',\n",
       " 'significa',\n",
       " 'significativa',\n",
       " 'significativo',\n",
       " 'sistema',\n",
       " 'sistemi',\n",
       " 'smartphone',\n",
       " 'sms',\n",
       " 'so',\n",
       " 'sociale',\n",
       " 'software',\n",
       " 'solamente',\n",
       " 'solo',\n",
       " 'sono',\n",
       " 'sorprende',\n",
       " 'source',\n",
       " 'spam',\n",
       " 'speciale',\n",
       " 'stanford',\n",
       " 'state',\n",
       " 'street',\n",
       " 'su',\n",
       " 'sua',\n",
       " 'suave',\n",
       " 'sue',\n",
       " 'sul',\n",
       " 'super',\n",
       " 'superando',\n",
       " 'support',\n",
       " 'survey',\n",
       " 'susningnu',\n",
       " 'svm',\n",
       " 'systems',\n",
       " 'sé',\n",
       " 't',\n",
       " 'tale',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tecnologia',\n",
       " 'ted',\n",
       " 'temo',\n",
       " 'tempo',\n",
       " 'tende',\n",
       " 'terabyte',\n",
       " 'terrence',\n",
       " 'terry',\n",
       " 'test',\n",
       " 'the',\n",
       " 'thomas',\n",
       " 'ti',\n",
       " 'time',\n",
       " 'tipo',\n",
       " 'to',\n",
       " 'today',\n",
       " 'toefl',\n",
       " 'tom',\n",
       " 'tono',\n",
       " 'tony',\n",
       " 'totalmente',\n",
       " 'training',\n",
       " 'tre',\n",
       " 'tremendamente',\n",
       " 'trimestre',\n",
       " 'triste',\n",
       " 'tu',\n",
       " 'tue',\n",
       " 'turing',\n",
       " 'turista',\n",
       " 'ultime',\n",
       " 'umm',\n",
       " 'un',\n",
       " 'una',\n",
       " 'universalmente',\n",
       " 'university',\n",
       " 'uno',\n",
       " 'usa',\n",
       " 'usando',\n",
       " 'usarlo',\n",
       " 'use',\n",
       " 'user',\n",
       " 'uso',\n",
       " 'utile',\n",
       " 'va',\n",
       " 'vai',\n",
       " 'vale',\n",
       " 'vandalismo',\n",
       " 'varie',\n",
       " 'vector',\n",
       " 'venir',\n",
       " 'verbale',\n",
       " 'verbo',\n",
       " 'vi',\n",
       " 'via',\n",
       " 'video',\n",
       " 'vista',\n",
       " 'visto',\n",
       " 'visualeditor',\n",
       " 'vocale',\n",
       " 'w',\n",
       " 'wales',\n",
       " 'war',\n",
       " 'ward',\n",
       " 'web',\n",
       " 'webby',\n",
       " 'weekend',\n",
       " 'whatsapp',\n",
       " 'white',\n",
       " 'wiki',\n",
       " 'wikibooks',\n",
       " 'wikimedia',\n",
       " 'wikinfo',\n",
       " 'wikipedia',\n",
       " 'wikipediacom',\n",
       " 'wikipediaorg',\n",
       " 'wikiquote',\n",
       " 'wikireader',\n",
       " 'wikisource',\n",
       " 'wikispecies',\n",
       " 'wikivoyage',\n",
       " 'wiktionary',\n",
       " 'william',\n",
       " 'wired',\n",
       " 'work',\n",
       " 'world',\n",
       " 'www',\n",
       " 'x',\n",
       " 'xx',\n",
       " 'yongle',\n",
       " 'you',\n",
       " 'youtube',\n",
       " 'zona',\n",
       " '\\xa0',\n",
       " '«una',\n",
       " '«wikipedia',\n",
       " 'ˌwɪkiˈpiːdiə',\n",
       " 'παιδεία',\n",
       " '–',\n",
       " '—'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we retrive all words that are present in both italian vocabolary and not italian vocabolary\n",
    "#these words are encoded with 2 in the words.csv file\n",
    "italian_words=set(words[words['where']==2]['Word'])\n",
    "italian_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each phrase in eng, we translate it and then see if there is a new word using bloom filter\n",
    "#pip install bloomfilter-py\n",
    "from bloomfilter import BloomFilter\n",
    "bloom_filter = BloomFilter(expected_insertions=len(italian_words)*2, err_rate=0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translation method\n",
    "\n",
    "def translate_text(text, tranlator):\n",
    "    out=tranlator.translate(text)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering method\n",
    "#if the word is not in the bloom filter,\n",
    "#we return true and add it to the bloom filter\n",
    "def handle_text(word,bloom_filter):\n",
    "    if word not in bloom_filter:\n",
    "        bloom_filter.put(word)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the english text into the single words\n",
    "def split_text(text):\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "def parse_text(text):\n",
    "        if text!=np.nan:\n",
    "                text=re.sub(r\"[!#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]\",'',text)  #punctuations\n",
    "                text = '' .join((z for z in text if not z.isdigit()))\n",
    "                \n",
    "                text=re.sub(r'\"','',text)\n",
    "                #special handling of the ' char\n",
    "                text=re.sub(r\"'\",' ',text)\n",
    "                text=re.sub(r'[[]]','',text)\n",
    "                text=text.lower()\n",
    "                text=text.strip()\n",
    "                #second processing of the text\n",
    "                text=text.translate(str.maketrans('','',string.punctuation))\n",
    "                return text\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1385\n"
     ]
    }
   ],
   "source": [
    "#LOGIC\n",
    "#we iterate all the english text  (A) and translate them in italian (B)\n",
    "#we check if the the words in (B) there is a word not present in the italian vocabolary\n",
    "# if B does not contain new words:\n",
    "#       B is not inserted as a new phrase of the dataset\n",
    "# else, if B contains at least a new word, then we insert B as a new italian phrase in the dataset\n",
    "# and update the italian vocabolary\n",
    "\n",
    "english_text=list(english_text)     # listing all the english phrases\n",
    "tranlator=GoogleTranslator(source='en',target='it') #initializing the Google translator obj\n",
    "new_text=[]     #list of all italian new phrases \n",
    "for text in english_text:\n",
    "    translated=translate_text(text,tranlator)   #we translate the phrase (B)\n",
    "    translated=parse_text(translated)\n",
    "    words=split_text(translated)        #we split B \n",
    "    for w in words:\n",
    "        if handle_text(w,bloom_filter): #we check if the current word (w) is new italian word or not\n",
    "\n",
    "            new_text.append(translated)     #if w is a new word (not present in the italian vocabolary) we append it to the list of new\n",
    "                                            #italian phrases\n",
    "            break\n",
    "\n",
    "print(len(new_text))    #we check the number of new phrases computed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text', 'Language', 'Class', 'Number of words', 'Number of spaces']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a new Dataframe containing the new italian phrases computed\n",
    "to_insert=[]\n",
    "for text in new_text:\n",
    "    number_of_words=len(text.split(' '))\n",
    "    number_of_spaces=number_of_words-1\n",
    "    label=1\n",
    "    lang='Italian'\n",
    "    entry=[text,lang,label,number_of_words,number_of_spaces]\n",
    "    to_insert.append(entry)\n",
    "\n",
    "columns=dataset.columns.to_list()[2:]\n",
    "columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we concatenate the old dataframe (not agumented) and the new dataframe (computed in the prev. cell)\n",
    "new_dataset=pd.DataFrame(data=to_insert,columns=columns)\n",
    "new_dataset=pd.concat([dataset,new_dataset],ignore_index=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2083"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we check the total of italian phrases present in the dataset used for training and evaluation\n",
    "italian_entry_numb=new_dataset[new_dataset.Class == 1].shape[0]\n",
    "italian_entry_numb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>Class</th>\n",
       "      <th>Number of words</th>\n",
       "      <th>Number of spaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7114</th>\n",
       "      <td>7114.0</td>\n",
       "      <td>7114.0</td>\n",
       "      <td>nature è una delle più antiche ed importanti r...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7115</th>\n",
       "      <td>7115.0</td>\n",
       "      <td>7115.0</td>\n",
       "      <td>viene pubblicata fin dal  novembre</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7116</th>\n",
       "      <td>7116.0</td>\n",
       "      <td>7116.0</td>\n",
       "      <td>nonostante la maggior parte delle riviste del ...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7117</th>\n",
       "      <td>7117.0</td>\n",
       "      <td>7117.0</td>\n",
       "      <td>molti sono gli avanzamenti e le scoperte prove...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7118</th>\n",
       "      <td>7118.0</td>\n",
       "      <td>7118.0</td>\n",
       "      <td>il fattore di impatto impact factor di questa ...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11717</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qual è stato il tuo errore ti diamo da mangiar...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11718</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>narcisa ha cambiato i suoi modi ha lottato all...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11719</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>come il narcisismo ora marian ha detto a entra...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11720</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ha lei immagino che non vorrebbe più pane d or...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11721</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>terry in realtà assomigli un po  a quell angel...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2083 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1  \\\n",
       "7114       7114.0        7114.0   \n",
       "7115       7115.0        7115.0   \n",
       "7116       7116.0        7116.0   \n",
       "7117       7117.0        7117.0   \n",
       "7118       7118.0        7118.0   \n",
       "...           ...           ...   \n",
       "11717         NaN           NaN   \n",
       "11718         NaN           NaN   \n",
       "11719         NaN           NaN   \n",
       "11720         NaN           NaN   \n",
       "11721         NaN           NaN   \n",
       "\n",
       "                                                    Text Language  Class  \\\n",
       "7114   nature è una delle più antiche ed importanti r...  Italian      1   \n",
       "7115                 viene pubblicata fin dal  novembre   Italian      1   \n",
       "7116   nonostante la maggior parte delle riviste del ...  Italian      1   \n",
       "7117   molti sono gli avanzamenti e le scoperte prove...  Italian      1   \n",
       "7118   il fattore di impatto impact factor di questa ...  Italian      1   \n",
       "...                                                  ...      ...    ...   \n",
       "11717  qual è stato il tuo errore ti diamo da mangiar...  Italian      1   \n",
       "11718  narcisa ha cambiato i suoi modi ha lottato all...  Italian      1   \n",
       "11719  come il narcisismo ora marian ha detto a entra...  Italian      1   \n",
       "11720  ha lei immagino che non vorrebbe più pane d or...  Italian      1   \n",
       "11721  terry in realtà assomigli un po  a quell angel...  Italian      1   \n",
       "\n",
       "       Number of words  Number of spaces  \n",
       "7114                28                27  \n",
       "7115                 7                 6  \n",
       "7116                41                40  \n",
       "7117                83                82  \n",
       "7118                17                16  \n",
       "...                ...               ...  \n",
       "11717               18                17  \n",
       "11718               41                40  \n",
       "11719               23                22  \n",
       "11720               14                13  \n",
       "11721               23                22  \n",
       "\n",
       "[2083 rows x 7 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italian_entry=new_dataset[new_dataset.Class==1]\n",
    "italian_entry\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking stats after agumentation with english phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11722\n",
      "Non-italian 9639\n",
      "Italian 2083\n",
      "Total: 11722 100%\n",
      "Not italian: 9639 82 %\n",
      "Italian: 2083 17 %\n"
     ]
    }
   ],
   "source": [
    "#Analyzing the cardinality of italian and not italian datapoints present in the dataset\n",
    "italian_entry=new_dataset[new_dataset.Class == 1].shape[0]\n",
    "other=new_dataset.shape[0]-italian_entry\n",
    "total=new_dataset.shape[0]\n",
    "print('total',total)\n",
    "print('Non-italian', other)\n",
    "print('Italian',italian_entry)\n",
    "\n",
    "print('Total:',total, '100%')\n",
    "\n",
    "print('Not italian:',other,int((other/total)*100),'%')\n",
    "print('Italian:',italian_entry,int((italian_entry/total)*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the new dataset\n",
    "new_dataset.to_csv('..\\\\..\\\\Dataset\\\\Dataset_agumented1.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French agumentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1014,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "french_text=dataset[dataset['Language']=='French']['Text']\n",
    "french_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n"
     ]
    }
   ],
   "source": [
    "french_text=list(french_text)     # listing all the english phrases\n",
    "tranlator=GoogleTranslator(source='fr',target='it') #initializing the Google translator obj\n",
    "new_text=[]     #list of all italian new phrases \n",
    "for text in french_text:\n",
    "    translated=translate_text(text,tranlator)   #we translate the phrase (B)\n",
    "    translated=parse_text(translated)\n",
    "    words=split_text(translated)        #we split B \n",
    "    for w in words:\n",
    "        if handle_text(w,bloom_filter): #we check if the current word (w) is new italian word or not\n",
    "\n",
    "            new_text.append(translated)     #if w is a new word (not present in the italian vocabolary) we append it to the list of new\n",
    "                                            #italian phrases\n",
    "            break\n",
    "\n",
    "print(len(new_text))    #we check the number of new phrases computed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text', 'Language', 'Class', 'Number of words', 'Number of spaces']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a new Dataframe containing the new italian phrases computed\n",
    "to_insert=[]\n",
    "for text in new_text:\n",
    "    number_of_words=len(text.split(' '))\n",
    "    number_of_spaces=number_of_words-1\n",
    "    label=1\n",
    "    lang='Italian'\n",
    "    entry=[text,lang,label,number_of_words,number_of_spaces]\n",
    "    to_insert.append(entry)\n",
    "\n",
    "columns=dataset.columns.to_list()[2:]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we concatenate the old dataframe (not agumented) and the new dataframe (computed in the prev. cell)\n",
    "new_dataset_fr=pd.DataFrame(data=to_insert,columns=columns)\n",
    "new_dataset=pd.concat([new_dataset_fr,new_dataset],ignore_index=True,axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12736\n",
      "Non-italian 9639\n",
      "Italian 3097\n",
      "Total: 12736 100%\n",
      "Not italian: 9639 75 %\n",
      "Italian: 3097 24 %\n"
     ]
    }
   ],
   "source": [
    "#Analyzing the cardinality of italian and not italian datapoints present in the dataset\n",
    "italian_entry=new_dataset[new_dataset.Class == 1].shape[0]\n",
    "other=new_dataset.shape[0]-italian_entry\n",
    "total=new_dataset.shape[0]\n",
    "print('total',total)\n",
    "print('Non-italian', other)\n",
    "print('Italian',italian_entry)\n",
    "\n",
    "print('Total:',total, '100%')\n",
    "\n",
    "print('Not italian:',other,int((other/total)*100),'%')\n",
    "print('Italian:',italian_entry,int((italian_entry/total)*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.to_csv('..\\\\..\\\\Dataset\\\\Dataset_agumented2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish agumentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(819,)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spanish_text=dataset[dataset['Language']=='Spanish']['Text']\n",
    "spanish_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n"
     ]
    }
   ],
   "source": [
    "spanish_text=list(spanish_text)     # listing all the spaish phrases\n",
    "tranlator=GoogleTranslator(source='es',target='it') #initializing the Google translator obj\n",
    "new_text=[]     #list of all italian new phrases \n",
    "for text in spanish_text:\n",
    "    if text!='':\n",
    "        try:\n",
    "            translated=translate_text(text,tranlator)   #we translate the phrase (B)\n",
    "            translated=parse_text(translated)\n",
    "            words=split_text(translated)        #we split B \n",
    "            for w in words:\n",
    "                if handle_text(w,bloom_filter): #we check if the current word (w) is new italian word or not\n",
    "\n",
    "                    new_text.append(translated)     #if w is a new word (not present in the italian vocabolary) we append it to the list of new\n",
    "                                                    #italian phrases\n",
    "                    break\n",
    "        except:\n",
    "            break\n",
    "\n",
    "print(len(new_text))    #we check the number of new phrases computed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot spanish phrase: 819\n",
      "Tot new italian phrases: 277\n"
     ]
    }
   ],
   "source": [
    "print('Tot spanish phrase:',len(spanish_text))\n",
    "print('Tot new italian phrases:',len(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text', 'Language', 'Class', 'Number of words', 'Number of spaces']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a new Dataframe containing the new italian phrases computed\n",
    "to_insert=[]\n",
    "for text in new_text:\n",
    "    number_of_words=len(text.split(' '))\n",
    "    number_of_spaces=number_of_words-1\n",
    "    label=1\n",
    "    lang='Italian'\n",
    "    entry=[text,lang,label,number_of_words,number_of_spaces]\n",
    "    to_insert.append(entry)\n",
    "\n",
    "columns=dataset.columns.to_list()[2:]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we concatenate the old dataframe (not agumented) and the new dataframe (computed in the prev. cell)\n",
    "new_dataset_sp=pd.DataFrame(data=to_insert,columns=columns)\n",
    "new_dataset=pd.concat([new_dataset_sp,new_dataset],ignore_index=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13013\n",
      "Non-italian 9639\n",
      "Italian 3374\n",
      "Total: 13013 100%\n",
      "Not italian: 9639 74 %\n",
      "Italian: 3374 25 %\n"
     ]
    }
   ],
   "source": [
    "#Analyzing the cardinality of italian and not italian datapoints present in the dataset\n",
    "italian_entry=new_dataset[new_dataset.Class == 1].shape[0]\n",
    "other=new_dataset.shape[0]-italian_entry\n",
    "total=new_dataset.shape[0]\n",
    "print('total',total)\n",
    "print('Non-italian', other)\n",
    "print('Italian',italian_entry)\n",
    "\n",
    "print('Total:',total, '100%')\n",
    "\n",
    "print('Not italian:',other,int((other/total)*100),'%')\n",
    "print('Italian:',italian_entry,int((italian_entry/total)*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.to_csv('..\\\\..\\\\Dataset\\\\Dataset_agumented3.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "#### The process of agumentation con be done also using other languages (german, ...). This will increase the number of italian phrase reducing the effect of training with unbalanced data.\n",
    "\n",
    "#### As the stats show \n",
    "#### English agumentation 6%->17% (+1385)\n",
    "#### French aguementation 17%->24% (+1014)\n",
    "#### Spanish agumentation 24%->25% (+277)\n",
    "#### The agumentation process has increased the number of italian phrases from 6% to 25%\n",
    "#### Better then before.\n",
    "#### This process could be extented to other languages such as Portoguese, German, Russian\n",
    "### Other options\n",
    "#### Rather then translating the existing not italian phrases, we could use transforms to compute for each italina phrases a semantically similar phrase\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
